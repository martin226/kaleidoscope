{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:08:22.259254Z","iopub.execute_input":"2023-01-01T18:08:22.260276Z","iopub.status.idle":"2023-01-01T18:08:24.271937Z","shell.execute_reply.started":"2023-01-01T18:08:22.26016Z","shell.execute_reply":"2023-01-01T18:08:24.270996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\npath = ''\n\ntokenizer = AutoTokenizer.from_pretrained(path)\nconfig = AutoConfig.from_pretrained(path)\nmodel = AutoModelForSequenceClassification.from_pretrained(path, config=config).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:13:00.669633Z","iopub.execute_input":"2023-01-01T18:13:00.670025Z","iopub.status.idle":"2023-01-01T18:13:02.935258Z","shell.execute_reply.started":"2023-01-01T18:13:00.669984Z","shell.execute_reply":"2023-01-01T18:13:02.934284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nds = load_dataset(\"go_emotions\", \"simplified\")\nvalid_ds = ds[\"validation\"]","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:09:40.089398Z","iopub.execute_input":"2023-01-01T18:09:40.089841Z","iopub.status.idle":"2023-01-01T18:09:45.057763Z","shell.execute_reply.started":"2023-01-01T18:09:40.0898Z","shell.execute_reply":"2023-01-01T18:09:45.056564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def one_hot_encode(example):\n    l = example[\"labels\"]\n    one_hot_list = [0] * (28)\n    for i in l:\n        one_hot_list[i] = 1\n    example[\"labels\"] = one_hot_list\n    return example","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ds = valid_ds.map(one_hot_encode)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_func(examples):\n  return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=50)","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:09:37.657184Z","iopub.execute_input":"2023-01-01T18:09:37.657559Z","iopub.status.idle":"2023-01-01T18:09:37.66336Z","shell.execute_reply.started":"2023-01-01T18:09:37.657524Z","shell.execute_reply":"2023-01-01T18:09:37.661981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ds = valid_ds.map(tokenize_func, batched=True, remove_columns=[\"text\", \"id\"])\nvalid_ds = valid_ds.rename_column(\"labels\", \"label\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_float_labels(example):\n    float_labels = example[\"label\"].to(torch.float)\n    example[\"float_label\"] = float_labels\n    return example","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ds.set_format(\"torch\")\nvalid_ds = valid_ds.map(to_float_labels).remove_columns(\"label\").rename_column(\"float_label\", \"label\")\ndataloader = torch.utils.data.DataLoader(valid_ds, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:10:21.156237Z","iopub.execute_input":"2023-01-01T18:10:21.156595Z","iopub.status.idle":"2023-01-01T18:10:21.1638Z","shell.execute_reply.started":"2023-01-01T18:10:21.156565Z","shell.execute_reply":"2023-01-01T18:10:21.162801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:13:13.092492Z","iopub.execute_input":"2023-01-01T18:13:13.092862Z","iopub.status.idle":"2023-01-01T18:13:13.103891Z","shell.execute_reply.started":"2023-01-01T18:13:13.092829Z","shell.execute_reply":"2023-01-01T18:13:13.102948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def temperature_scale(logits, temperature):\n    # Expand temperature to match the size of logits\n    temperature = temperature.unsqueeze(0).expand(logits.size(0), logits.size(1))\n    return logits / temperature","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:30:14.04771Z","iopub.execute_input":"2023-01-01T18:30:14.048092Z","iopub.status.idle":"2023-01-01T18:30:14.053221Z","shell.execute_reply.started":"2023-01-01T18:30:14.048059Z","shell.execute_reply":"2023-01-01T18:30:14.052028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_optimal_temperature(model, valid_loader, initial_temp=1.5, max_iter=10000):\n    temp = torch.nn.Parameter(torch.tensor(initial_temp, dtype=torch.float, requires_grad=True, device=device))\n    \n    nll_criterion = nn.BCEWithLogitsLoss().to(device)\n\n    # First: collect all the logits and labels for the validation set\n    logits_list = []\n    labels_list = []\n    with torch.no_grad():\n        for batch in valid_loader:\n            enc = {\n                'input_ids': batch['input_ids'].to(device),\n                'attention_mask': batch['attention_mask'].to(device),\n            }\n            logits = model(**enc).logits\n            logits_list.append(logits)\n            labels_list.append(batch['label'])\n        logits = torch.cat(logits_list).to(device)\n        labels = torch.cat(labels_list).to(device)\n\n    # Calculate NLL and ECE before temperature scaling\n    before_temperature_nll = nll_criterion(logits, labels).item()\n    print('Before temperature - NLL: %.3f' % (before_temperature_nll))\n\n    # Next: optimize the temperature w.r.t. NLL\n    optimizer = optim.LBFGS([temp], lr=0.01, max_iter=max_iter)\n\n    def eval():\n        optimizer.zero_grad()\n        loss = nll_criterion(temperature_scale(logits, temp), labels)\n        loss.backward()\n        return loss\n    optimizer.step(eval)\n\n    # Calculate NLL and ECE after temperature scaling\n    after_temperature_nll = nll_criterion(temperature_scale(logits, temp), labels).item()\n    print('Optimal temperature: %.3f' % temp.item())\n    print('After temperature - NLL: %.3f' % (after_temperature_nll))\n\n    return temp.item()","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:30:15.952204Z","iopub.execute_input":"2023-01-01T18:30:15.952559Z","iopub.status.idle":"2023-01-01T18:30:15.965715Z","shell.execute_reply.started":"2023-01-01T18:30:15.95253Z","shell.execute_reply":"2023-01-01T18:30:15.964648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_optimal_temperature(model, dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-01-01T18:30:19.960624Z","iopub.execute_input":"2023-01-01T18:30:19.961013Z","iopub.status.idle":"2023-01-01T18:30:26.437924Z","shell.execute_reply.started":"2023-01-01T18:30:19.960962Z","shell.execute_reply":"2023-01-01T18:30:26.436951Z"},"trusted":true},"execution_count":null,"outputs":[]}]}